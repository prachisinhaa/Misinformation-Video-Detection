{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrVA4GFDKQDb",
        "outputId": "58320720-922b-42ae-d6d2-174b2a7b17af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B2n9UfCez_KW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "image_folder=\"/content/drive/MyDrive/output\"\n",
        "\n",
        "filtered_annotations_df = pd.read_csv('/content/filtered_annotations.csv')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDwY-EAGWgY_"
      },
      "source": [
        "This code snippet is processing annotations and images.\n",
        "1. Imports:\n",
        "os: Provides a way to interact with the operating system, like reading file paths.\n",
        "cv2: OpenCV library for computer vision tasks.\n",
        "pandas as pd: Pandas library for data manipulation.\n",
        "2. An empty list data is initialized to store tuples of image and its corresponding annotation.\n",
        "3. The loop iterates over each row of the filtered_annotations_df DataFrame. The DataFrame contains annotations for videos. For each row, it extracts the video_id and normalized_annotation.\n",
        "4. Checking if the image at image_path exists. If it does, it reads the image using OpenCV's imread function and stores it in the image variable.\n",
        "5. If the image was successfully read (i.e., not None), it appends a tuple containing the image and its annotation to the data list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0CCYNd1e5hWn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "\n",
        "data = []\n",
        "\n",
        "for index, row in filtered_annotations_df.iterrows():\n",
        "    video_id = row['video_id']  # Extract video ID\n",
        "    annotation = row['normalized_annotation']  # Extract annotation\n",
        "\n",
        "    # Assuming image folder contains images with names corresponding to video IDs\n",
        "    image_path = os.path.join(image_folder, f\"{video_id}.jpg\")  # Construct image path\n",
        "    if os.path.exists(image_path):  # Check if image exists\n",
        "        image = cv2.imread(image_path)  # Read image using OpenCV\n",
        "        if image is not None:  # Check if image is successfully read\n",
        "            data.append((image, annotation))  # Append image and annotation to the data list\n",
        "\n",
        "# Now 'data' contains tuples of (image, annotation), where image is the image array and annotation is its corresponding label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWDp24JIZXo9"
      },
      "source": [
        "Printing the size of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMKmL_H97qv0",
        "outputId": "72b14bbd-0773-4aef-b9ba-1440a84b5e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the data: 783\n"
          ]
        }
      ],
      "source": [
        "data_size = len(data)\n",
        "print(\"Size of the data:\", data_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvaZ99MzZapC"
      },
      "source": [
        "X= images Y= annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV5RNT2o7unN"
      },
      "outputs": [],
      "source": [
        "X = [item[0] for item in data]\n",
        "Y = [item[1] for item in data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYCBlj58Zd9S"
      },
      "source": [
        "Convert labels from -1, 0, 1 to 0, 1, 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX0ayVm272t8"
      },
      "outputs": [],
      "source": [
        "Y = [label + 1 for label in Y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saZXdOWw8dgb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming data is a list of tuples (image, annotation)\n",
        "\n",
        "# Extract features (images) and labels (annotations)\n",
        "X = np.array([np.ravel(image) for image, _ in data])  # Flatten each image into a one-dimensional array\n",
        "y = np.array([annotation for _, annotation in data])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTATION OF THREE CLASSIFIERS:\n",
        "1. LOGISTIC REGRESSION\n",
        "2. NAIVE BAYES CLASSIFIER\n",
        "3. RANDOM FOREST CLASSIFIER"
      ],
      "metadata": {
        "id": "QmLYF5PsbZox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are your training and testing data\n",
        "# Define a list of tuples containing model name and model instance\n",
        "models = [\n",
        "    ('LR', LogisticRegression()),\n",
        "    ('NB', GaussianNB()),\n",
        "    ('RF', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest classifier\n",
        "]\n",
        "\n",
        "# Create an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over each model\n",
        "for name, model in models:\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Append results to list\n",
        "    results.append((name, accuracy, precision, recall, f1))\n",
        "\n",
        "# Convert the list of results into a DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "\n",
        "# Print the results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "h5M2hlGDGWTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c63a6c36-8edf-4383-efce-25c710233aed"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Model  Accuracy  Precision    Recall  F1 Score\n",
            "0    LR  0.496815   0.519095  0.496815  0.499299\n",
            "1    NB  0.426752   0.459230  0.426752  0.438464\n",
            "2    RF  0.496815   0.405883  0.496815  0.411392\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
